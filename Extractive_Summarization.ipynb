{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "MErqTNVzB1Vm",
        "G1P_W0--Qfpo",
        "mhkQx85Jnkez",
        "EDzzCg3c23qA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **OpenAI Whisper**"
      ],
      "metadata": {
        "id": "MErqTNVzB1Vm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrvqnvkmI7LT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8641eb33-6f48-4809-f4be-6079f453efd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-17nfr1du\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-17nfr1du\n",
            "  Resolved https://github.com/openai/whisper.git to commit 0a60fcaa9b86748389a656aa013c416030287d47\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (2.0.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (0.56.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (10.1.0)\n",
            "Collecting tiktoken==0.3.3 (from openai-whisper==20230918)\n",
            "  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230918) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230918) (2.31.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230918) (3.27.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230918) (3.12.4)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230918) (17.0.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230918) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230918) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20230918) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20230918) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20230918-py3-none-any.whl size=798399 sha256=c849d13d613d73423b1531d84016aaedddcad1d2c373c13702e631f4d4805867\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-w5rvl197/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20230918 tiktoken-0.3.3\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,266 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,342 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 2,950 kB in 4s (790 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "18 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper \"/content/Seminar_In_Bangkok_Jan25.mp3\" --model medium.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XSWP4myKw6G",
        "outputId": "bf3d56a4-c1aa-41f7-d81e-aae4118a5c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 82%|██████████████████████████████▍      | 1.17G/1.42G [01:25<00:18, 14.8MiB/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/whisper\", line 8, in <module>\n",
            "    sys.exit(cli())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\", line 444, in cli\n",
            "    model = load_model(model_name, device=device, download_root=model_dir)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/__init__.py\", line 131, in load_model\n",
            "    checkpoint_file = _download(_MODELS[name], download_root, in_memory)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/__init__.py\", line 76, in _download\n",
            "    buffer = source.read(8192)\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 466, in read\n",
            "    s = self.fp.read(amt)\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/lib/python3.10/ssl.py\", line 1274, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "  File \"/usr/lib/python3.10/ssl.py\", line 1130, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"medium\")\n",
        "#result = model.transcribe(\"/content/Seminar_In_Bangkok_Jan25.mp3\")\n",
        "#print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "QqvN8XaefQCn",
        "outputId": "78f422c6-945f-4605-d6ca-aba58c64088d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6eabda7ff71f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"medium\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#result = model.transcribe(\"/content/Seminar_In_Bangkok_Jan25.mp3\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(result[\"text\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'whisper'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "jLWjpOLGBsDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(model, open('/content/audio_Text.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "GIWrKzUvBflC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text_into_lines(text, line_limit):\n",
        "    words = text.split()\n",
        "    lines = []\n",
        "    current_line = \"\"\n",
        "    for word in words:\n",
        "        if len(current_line) + len(word) + 1 <= line_limit:\n",
        "            current_line += word + \" \"\n",
        "        else:\n",
        "            lines.append(current_line)\n",
        "            current_line = word + \" \"\n",
        "\n",
        "    # Add the last line\n",
        "    if current_line:\n",
        "        lines.append(current_line)\n",
        "\n",
        "    return lines"
      ],
      "metadata": {
        "id": "DOhNSrbgfYj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text['text']\n",
        "text_output_file_path = \"/content/Text.txt\"\n",
        "line_limit = 80\n",
        "lines = split_text_into_lines(result['text'], line_limit)\n",
        "with open(text_output_file_path, \"w\") as text_file:\n",
        "      for line in lines:\n",
        "        text_file.write(line + '\\n')"
      ],
      "metadata": {
        "id": "aiyBZQaKfWDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing"
      ],
      "metadata": {
        "id": "mhkQx85Jnkez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMcU1Hq1Uv2C",
        "outputId": "851f129a-da43-41df-ea27-e1b8c48664e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk"
      ],
      "metadata": {
        "id": "ReDLEL99WZj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKg2MxJBnwsX",
        "outputId": "d68d3bbf-078b-45c1-c7e8-4afb7f2c0d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = \"Text.txt\"\n",
        "with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file:\n",
        "    text1 = input_file.read()"
      ],
      "metadata": {
        "id": "UpjPgNswWaqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentences = sent_tokenize(text1)\n",
        "\n",
        "# Lemmatization and Stemming\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Preprocessing for each sentence\n",
        "preprocessed_sentences = []\n",
        "before_proprocessed=[]\n",
        "for sentence in sentences:\n",
        "    sentence=sentence.replace('\\n', '')\n",
        "    before_proprocessed.append(sentence)\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # Remove special characters and digits\n",
        "    sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
        "\n",
        "    # Tokenize the sentence into words\n",
        "    words = word_tokenize(sentence)\n",
        "\n",
        "    # Remove stopwords\n",
        "    filtered_words = [word for word in words if word not in stopwords.words(\"english\")]\n",
        "\n",
        "    # Stemming\n",
        "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "    # Join the lemmatized words back into a sentence\n",
        "    preprocessed_sentence = \" \".join(lemmatized_words)\n",
        "\n",
        "    preprocessed_sentences.append(preprocessed_sentence)\n",
        "\n",
        "print(preprocessed_sentences)\n",
        "\n",
        "\"\"\"# Write the preprocessed text to a new file\n",
        "preprocessed_output_file_path = \"preprocessed_text.txt\"\n",
        "with open(preprocessed_output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
        "    output_file.write(preprocessed_text)\n",
        "\n",
        "print(\"Preprocessed text saved to:\", preprocessed_output_file_path)\"\"\"\n",
        "#print(before_proprocessed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woZ_3wlwXEt3",
        "outputId": "d6c13033-9a97-47e0-d424-2ab7f2c718fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello aj', 'exciting news', 'seminar bangkok first effortless english breakthrough seminar bangkok thailand january th', 'im telling everybody even dont live bangkok developing seminar future visit many city around world', 'example summer definitely coming japan well effortless english breakthrough seminar japan probably kansai area kyoto osaka possibly kobe', 'right first seminar bangkok thailand', 'first seminar january th', 'january th starting pm thats sunday january th', 'royal hotel number ratchadamnurn', 'thats across sanam luang across big field', 'anyway seminar im going number one im going talk effortless english system especially new thing focusing system', 'know many story know using know listen first focusing input', 'ill talk people might know', 'may new people', 'im also going talk new thing adding system order make system even powerful', 'new element especially powerful live person lot interaction', 'lot energy room together learning together', 'im going talk new element effortless english system', 'im going talk general element effortless english system learn english quickly easily effectively speak easily fast fluently without thinking', 'im going actually demonstration lesson including mini story lesson point view lesson', 'remember point view lesson lesson teach grammar natural way intuitive way', 'student never think grammar rule still learn grammar learn like native speaker', 'thats power point view lesson', 'well experience live mini story lesson point view lesson powerful lesson type', 'well live person', 'lot fun together room create story together', 'youre listening tell story ask question', 'youre actually shouting answer change story answer', 'lot fun', 'there lot energy room', 'come one seminar definitely come', 'good news', 'right seminar free right developing', 'im testing different part seminar', 'want create something amazing remarkable outstanding', 'eventually develop longer seminar possibly two three maybe four day intensive english learning experience', 'time charge money', 'youll pay well need get bigger conference room hotel', 'well pay lot money', 'itll become expensive', 'right great opportunity one day', 'really half day seminar get quick breakthrough english', 'free right', 'thats great', 'live bangkok southeast asia come seminar', 'going first one january th sunday', 'go website effortlessenglishclubcom information', 'well another one february another one march also bangkok', 'live thailand get bangkok check website', 'ill announce new seminar schedule', 'right first one january th come bangkok experience effortless english live', 'learn personally live', 'okay hope see', 'great day enjoy english learning start new year powerful english learning powerful motivation powerful goal', 'see next time', 'bye bye']\n",
            "['Hello, this is AJ.', 'I have some very exciting news.', 'We are having our seminar in Bangkok, our first effortless English breakthrough seminar in Bangkok, Thailand on January 25th.', \"Now I'm telling everybody about this, even if you don't live in Bangkok, because we are developing these seminars so that in the future we can visit many cities around the world.\", \"For example, this summer we will definitely be coming to Japan and we'll be doing effortless English breakthrough seminars in Japan, probably in the Kansai area in Kyoto, Osaka, possibly Kobe.\", \"But right now our first seminars we're doing in Bangkok, Thailand.\", 'And our very first seminar is January 25th.', \"So this January 25th starting at 1045 until 4 p.m. and that's on a Sunday, January 25th.\", 'It will be at the Royal Hotel, which is at number 2, Ratchadamnurn.', \"And that's across from Sanam Luang, across from the big field.\", \"But anyway, at these seminars what I'm going to do is number one, I'm going to talk about the effortless English system and especially about some new things that we are focusing on with our system.\", 'So you know about the many stories, you know about using, you know, listen first and focusing on input.', \"And I'll talk some about that too because some people might not know about it.\", 'There may be some new people there.', \"But I'm also going to talk about some of the new things that we are adding to our system in order to make the system even more powerful.\", 'And these new elements are especially powerful live in person with me because we can have a lot of interaction.', \"There can be a lot more energy when we're all in the same room together, learning together.\", \"So I'm going to talk about these new elements of the effortless English system.\", \"And I'm going to talk about the general elements of the effortless English system, how to learn English most quickly, most easily, most effectively, how to speak easily, fast and fluently without thinking.\", \"And then I'm going to actually do some demonstration lessons, including a mini story lesson and a point of view lesson.\", 'And remember the point of view lessons, those are the lessons where I teach grammar in a natural way, in an intuitive way.', 'So you, the student, you never think about grammar rules, but you still learn the grammar, but you learn it like a native speaker.', \"That's the power of those point of view lessons.\", \"So we'll experience them live, the mini story lessons and the point of view lessons, which are the most powerful lesson types that we have.\", \"And we'll do it live in person.\", \"It's a lot more fun when we're all together in a room because we create the stories together.\", \"You're not just listening to me tell a story and ask questions.\", \"You're actually shouting out answers and I change the story with your answers.\", \"It's a lot of fun.\", \"There's a lot of energy in the room.\", 'So if you can come to one of our seminars, definitely come.', 'More good news.', \"Right now, the seminars are free because right now we're just developing them.\", \"I'm testing different parts of the seminar.\", 'I want to create something that is amazing, that is remarkable, that is outstanding.', 'Now, eventually, we will develop a longer seminar that will be possibly two, three, maybe four days, a very intensive English learning experience.', 'At that time, we will charge money.', \"You'll have to pay because we'll need to get a bigger conference room at a hotel.\", \"We'll have to pay a lot more money.\", \"So it'll become more expensive.\", \"But right now is a great opportunity because it's just a one day.\", \"Really, it's a half day seminar just to get a quick breakthrough with your English.\", \"And it's free right now.\", \"So that's great.\", 'So if you live in Bangkok or in Southeast Asia, come to our seminars.', \"We're going to have the first one January 25th on Sunday.\", 'Go to our website, effortlessenglishclub.com for more information.', \"We'll have another one in February and another one in March, also in Bangkok.\", 'So if you live in Thailand or you can get to Bangkok, check our website.', \"I'll announce the new seminars when we schedule them.\", 'Right now, the first one, January 25th, come to Bangkok and experience effortless English live.', 'Learn with me personally, live.', 'Okay, I hope to see you there.', 'And have a great day and enjoy your English learning and start this new year with powerful English learning, powerful motivation, powerful goals.', 'See you next time.', 'Bye bye.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction"
      ],
      "metadata": {
        "id": "EDzzCg3c23qA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Vector Initialisation\n",
        "feature_vector=[[] for i in range(len(preprocessed_sentences))]"
      ],
      "metadata": {
        "id": "qyNUq-zzGaSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.Title Feature\n",
        "\n",
        "title=\"Seminar In Bangkok Jan25\"\n",
        "title = title.lower()\n",
        "title = re.sub(r'[^a-zA-Z\\s]', '', title)\n",
        "title_sentences = sent_tokenize(title)\n",
        "title_words = [word_tokenize(sentence) for sentence in title_sentences][0]\n",
        "print(title_words)\n",
        "\n",
        "for i in range(len(preprocessed_sentences)):\n",
        "  w = word_tokenize(preprocessed_sentences[i])\n",
        "  common_words = [word for word in w if word in title_words]\n",
        "  score = len(common_words) / len(title_words) if len(title_words) > 0 else 0\n",
        "  feature_vector[i].append(score)\n",
        "feature_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfUQESpZ27qp",
        "outputId": "5f08778a-6bab-470b-c191-bd99f480d180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['seminar', 'in', 'bangkok', 'jan']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.0],\n",
              " [0.0],\n",
              " [1.0],\n",
              " [0.5],\n",
              " [0.25],\n",
              " [0.5],\n",
              " [0.25],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.25],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.25],\n",
              " [0.0],\n",
              " [0.25],\n",
              " [0.25],\n",
              " [0.0],\n",
              " [0.25],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.25],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.5],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.25],\n",
              " [0.25],\n",
              " [0.25],\n",
              " [0.25],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0],\n",
              " [0.0]]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.Sentence Length\n",
        "\n",
        "max_sentence_length = max(len(word_tokenize(sentence)) for sentence in preprocessed_sentences)\n",
        "for i in range(len(preprocessed_sentences)):\n",
        "    w = word_tokenize(preprocessed_sentences[i])\n",
        "    sentence_length = len(w)\n",
        "    length_score = sentence_length / max_sentence_length if max_sentence_length > 0 else 0\n",
        "    feature_vector[i].append(length_score)\n",
        "\n",
        "feature_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A6erUvuHayO",
        "outputId": "d01edf59-67a7-465e-b981-ead2984395c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.0, 0.10526315789473684],\n",
              " [0.0, 0.10526315789473684],\n",
              " [1.0, 0.5789473684210527],\n",
              " [0.5, 0.7894736842105263],\n",
              " [0.25, 0.9473684210526315],\n",
              " [0.5, 0.2631578947368421],\n",
              " [0.25, 0.21052631578947367],\n",
              " [0.0, 0.42105263157894735],\n",
              " [0.0, 0.21052631578947367],\n",
              " [0.0, 0.3684210526315789],\n",
              " [0.25, 0.8947368421052632],\n",
              " [0.0, 0.5263157894736842],\n",
              " [0.0, 0.2631578947368421],\n",
              " [0.0, 0.15789473684210525],\n",
              " [0.0, 0.6842105263157895],\n",
              " [0.0, 0.42105263157894735],\n",
              " [0.0, 0.3157894736842105],\n",
              " [0.0, 0.42105263157894735],\n",
              " [0.0, 1.0],\n",
              " [0.0, 0.631578947368421],\n",
              " [0.0, 0.5789473684210527],\n",
              " [0.0, 0.631578947368421],\n",
              " [0.0, 0.2631578947368421],\n",
              " [0.0, 0.631578947368421],\n",
              " [0.0, 0.15789473684210525],\n",
              " [0.0, 0.3684210526315789],\n",
              " [0.0, 0.3157894736842105],\n",
              " [0.0, 0.3684210526315789],\n",
              " [0.0, 0.10526315789473684],\n",
              " [0.0, 0.21052631578947367],\n",
              " [0.25, 0.2631578947368421],\n",
              " [0.0, 0.10526315789473684],\n",
              " [0.25, 0.2631578947368421],\n",
              " [0.25, 0.2631578947368421],\n",
              " [0.0, 0.3157894736842105],\n",
              " [0.25, 0.7368421052631579],\n",
              " [0.0, 0.15789473684210525],\n",
              " [0.0, 0.47368421052631576],\n",
              " [0.0, 0.21052631578947367],\n",
              " [0.0, 0.15789473684210525],\n",
              " [0.0, 0.2631578947368421],\n",
              " [0.25, 0.42105263157894735],\n",
              " [0.0, 0.10526315789473684],\n",
              " [0.0, 0.10526315789473684],\n",
              " [0.5, 0.3157894736842105],\n",
              " [0.0, 0.3157894736842105],\n",
              " [0.0, 0.21052631578947367],\n",
              " [0.25, 0.47368421052631576],\n",
              " [0.25, 0.3157894736842105],\n",
              " [0.25, 0.2631578947368421],\n",
              " [0.25, 0.5789473684210527],\n",
              " [0.0, 0.15789473684210525],\n",
              " [0.0, 0.15789473684210525],\n",
              " [0.0, 0.7894736842105263],\n",
              " [0.0, 0.15789473684210525],\n",
              " [0.0, 0.10526315789473684]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.Term Weight\n",
        "\"\"\"from collections import Counter\n",
        "import math\n",
        "sentence_words = [word_tokenize(sentence) for sentence in preprocessed_sentences]\n",
        "\n",
        "# Calculate the total number of sentences\n",
        "N = len(preprocessed_sentences)\n",
        "sentence_features=[]\n",
        "for i in range(len(sentence_words)):\n",
        "    #frequency of each word within the sentence\n",
        "    word_frequencies = Counter(sentence_words[i])\n",
        "\n",
        "    # Calculate the number of sentences each word occurs in\n",
        "    word_sentence_occurrences = {word: sum(1 for other_words in sentence_words if word in other_words) for word in set(sentence_words[i])}\n",
        "\n",
        "    #IDF Score\n",
        "    sentence_feature = sum(word_frequencies[word] * math.log(N / word_sentence_occurrences[word]) for word in sentence_words[i])\n",
        "    feature_vector[i].append(sentence_feature)\n",
        "print(feature_vector)\"\"\"\n"
      ],
      "metadata": {
        "id": "lMhSdgAmH-YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(preprocessed_sentences)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "idf_values = vectorizer.idf_\n",
        "\n",
        "# dictionary to store term IDF scores\n",
        "term_idf_scores = {term: idf for term, idf in zip(feature_names, idf_values)}\n",
        "\n",
        "for i in range(len(preprocessed_sentences)):\n",
        "  sc=0\n",
        "  w = word_tokenize(preprocessed_sentences[i])\n",
        "  l=[term_idf_scores[j] for j in w]\n",
        "  sc+=[term_idf_scores[j] for j in w][0]\n",
        "  score=sc/max(l)\n",
        "  feature_vector[i].append(score)\n",
        "\n",
        "print(feature_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QB7QW8YtR5B4",
        "outputId": "22fa7e93-c12e-4e00-d41f-0ed8bf9e0bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0, 0.10526315789473684, 1.0], [0.0, 0.10526315789473684, 1.0], [1.0, 0.5789473684210527, 0.6574114712971478], [0.5, 0.7894736842105263, 0.6813046142384115], [0.25, 0.9473684210526315, 1.0], [0.5, 0.2631578947368421, 0.8891189328545094], [0.25, 0.21052631578947367, 0.9525878668001048], [0.0, 0.42105263157894735, 0.7474398822075096], [0.0, 0.21052631578947367, 1.0], [0.0, 0.3684210526315789, 0.7893538079253951], [0.25, 0.8947368421052632, 1.0], [0.0, 0.5263157894736842, 0.9067875750883038], [0.0, 0.2631578947368421, 0.9067875750883038], [0.0, 0.15789473684210525, 1.0], [0.0, 0.6842105263157895, 0.6813046142384115], [0.0, 0.42105263157894735, 0.6813046142384115], [0.0, 0.3157894736842105, 0.7851917941024255], [0.0, 0.42105263157894735, 0.8104475637176851], [0.0, 1.0, 0.6813046142384115], [0.0, 0.631578947368421, 0.6813046142384115], [0.0, 0.5789473684210527, 1.0], [0.0, 0.631578947368421, 1.0], [0.0, 0.2631578947368421, 0.7893538079253951], [0.0, 0.631578947368421, 0.712002162953373], [0.0, 0.15789473684210525, 0.7851917941024255], [0.0, 0.3684210526315789, 0.7851917941024255], [0.0, 0.3157894736842105, 0.9067875750883038], [0.0, 0.3684210526315789, 0.9067875750883038], [0.0, 0.10526315789473684, 0.7851917941024255], [0.0, 0.21052631578947367, 1.0], [0.25, 0.2631578947368421, 0.9270664157891029], [0.0, 0.10526315789473684, 1.0], [0.25, 0.2631578947368421, 0.824272302291662], [0.25, 0.2631578947368421, 0.6813046142384115], [0.0, 0.3157894736842105, 1.0], [0.25, 0.7368421052631579, 1.0], [0.0, 0.15789473684210525, 0.9067875750883038], [0.0, 0.47368421052631576, 1.0], [0.0, 0.21052631578947367, 0.7851917941024255], [0.0, 0.15789473684210525, 1.0], [0.0, 0.2631578947368421, 0.7474398822075096], [0.25, 0.42105263157894735, 1.0], [0.0, 0.10526315789473684, 1.0], [0.0, 0.10526315789473684, 0.9389777453063762], [0.5, 0.3157894736842105, 0.6542274572958134], [0.0, 0.3157894736842105, 0.7851917941024255], [0.0, 0.21052631578947367, 1.0], [0.25, 0.47368421052631576, 0.712002162953373], [0.25, 0.3157894736842105, 0.6542274572958134], [0.25, 0.2631578947368421, 0.9067875750883038], [0.25, 0.5789473684210527, 0.8891189328545094], [0.0, 0.15789473684210525, 0.8406523071192057], [0.0, 0.15789473684210525, 1.0], [0.0, 0.7894736842105263, 0.8406523071192057], [0.0, 0.15789473684210525, 0.9067875750883038], [0.0, 0.10526315789473684, 1.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.Sentence Position\n",
        "\n",
        "first_5_sentences = preprocessed_sentences[:5]\n",
        "\n",
        "#S_F4 Score\n",
        "s_f4_scores = []\n",
        "for i, sentence in enumerate(first_5_sentences):\n",
        "    s_f4_score = (5 - i) / 5\n",
        "    feature_vector[i].append(s_f4_score)\n",
        "#0 for other sentances\n",
        "for i in range(5,len(preprocessed_sentences)):\n",
        "  feature_vector[i].append(0)\n",
        "print(feature_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga6-PAQGMK8h",
        "outputId": "aac76566-4560-46f7-ac25-acbd52c1850b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0, 0.10526315789473684, 1.0, 1.0], [0.0, 0.10526315789473684, 1.0, 0.8], [1.0, 0.5789473684210527, 0.6574114712971478, 0.6], [0.5, 0.7894736842105263, 0.6813046142384115, 0.4], [0.25, 0.9473684210526315, 1.0, 0.2], [0.5, 0.2631578947368421, 0.8891189328545094, 0], [0.25, 0.21052631578947367, 0.9525878668001048, 0], [0.0, 0.42105263157894735, 0.7474398822075096, 0], [0.0, 0.21052631578947367, 1.0, 0], [0.0, 0.3684210526315789, 0.7893538079253951, 0], [0.25, 0.8947368421052632, 1.0, 0], [0.0, 0.5263157894736842, 0.9067875750883038, 0], [0.0, 0.2631578947368421, 0.9067875750883038, 0], [0.0, 0.15789473684210525, 1.0, 0], [0.0, 0.6842105263157895, 0.6813046142384115, 0], [0.0, 0.42105263157894735, 0.6813046142384115, 0], [0.0, 0.3157894736842105, 0.7851917941024255, 0], [0.0, 0.42105263157894735, 0.8104475637176851, 0], [0.0, 1.0, 0.6813046142384115, 0], [0.0, 0.631578947368421, 0.6813046142384115, 0], [0.0, 0.5789473684210527, 1.0, 0], [0.0, 0.631578947368421, 1.0, 0], [0.0, 0.2631578947368421, 0.7893538079253951, 0], [0.0, 0.631578947368421, 0.712002162953373, 0], [0.0, 0.15789473684210525, 0.7851917941024255, 0], [0.0, 0.3684210526315789, 0.7851917941024255, 0], [0.0, 0.3157894736842105, 0.9067875750883038, 0], [0.0, 0.3684210526315789, 0.9067875750883038, 0], [0.0, 0.10526315789473684, 0.7851917941024255, 0], [0.0, 0.21052631578947367, 1.0, 0], [0.25, 0.2631578947368421, 0.9270664157891029, 0], [0.0, 0.10526315789473684, 1.0, 0], [0.25, 0.2631578947368421, 0.824272302291662, 0], [0.25, 0.2631578947368421, 0.6813046142384115, 0], [0.0, 0.3157894736842105, 1.0, 0], [0.25, 0.7368421052631579, 1.0, 0], [0.0, 0.15789473684210525, 0.9067875750883038, 0], [0.0, 0.47368421052631576, 1.0, 0], [0.0, 0.21052631578947367, 0.7851917941024255, 0], [0.0, 0.15789473684210525, 1.0, 0], [0.0, 0.2631578947368421, 0.7474398822075096, 0], [0.25, 0.42105263157894735, 1.0, 0], [0.0, 0.10526315789473684, 1.0, 0], [0.0, 0.10526315789473684, 0.9389777453063762, 0], [0.5, 0.3157894736842105, 0.6542274572958134, 0], [0.0, 0.3157894736842105, 0.7851917941024255, 0], [0.0, 0.21052631578947367, 1.0, 0], [0.25, 0.47368421052631576, 0.712002162953373, 0], [0.25, 0.3157894736842105, 0.6542274572958134, 0], [0.25, 0.2631578947368421, 0.9067875750883038, 0], [0.25, 0.5789473684210527, 0.8891189328545094, 0], [0.0, 0.15789473684210525, 0.8406523071192057, 0], [0.0, 0.15789473684210525, 1.0, 0], [0.0, 0.7894736842105263, 0.8406523071192057, 0], [0.0, 0.15789473684210525, 0.9067875750883038, 0], [0.0, 0.10526315789473684, 1.0, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.Sentence to Sentence Similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "#Cosine Similarity\n",
        "cosine_similarities = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "for i, sentence in enumerate(preprocessed_sentences):\n",
        "    #summary of sentence similarity for this sentence\n",
        "    sentence_similarity = sum(cosine_similarities[i, j] for j in range(len(preprocessed_sentences)) if j != i)\n",
        "\n",
        "    max_summary_similarity = sum(max(cosine_similarities[j, k] for k in range(len(preprocessed_sentences)) if k != j) for j in range(len(preprocessed_sentences)))\n",
        "\n",
        "    feature_score = sentence_similarity / max_summary_similarity if max_summary_similarity > 0 else 0\n",
        "    feature_vector[i].append(feature_score)\n",
        "\n",
        "print(feature_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9hrF3GGNPVw",
        "outputId": "904ad822-228b-43b1-b15d-e44d933b9a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0, 0.10526315789473684, 1.0, 1.0, 0.0], [0.0, 0.10526315789473684, 1.0, 0.8, 0.02086418180020698], [1.0, 0.5789473684210527, 0.6574114712971478, 0.6, 0.22720867015172186], [0.5, 0.7894736842105263, 0.6813046142384115, 0.4, 0.08956937112594304], [0.25, 0.9473684210526315, 1.0, 0.2, 0.07124723978793039], [0.5, 0.2631578947368421, 0.8891189328545094, 0, 0.18309205162308462], [0.25, 0.21052631578947367, 0.9525878668001048, 0, 0.16839073908180144], [0.0, 0.42105263157894735, 0.7474398822075096, 0, 0.1002796667282229], [0.0, 0.21052631578947367, 1.0, 0, 0.012720739995361378], [0.0, 0.3684210526315789, 0.7893538079253951, 0, 0.017245274228144283], [0.25, 0.8947368421052632, 1.0, 0, 0.17861757764373995], [0.0, 0.5263157894736842, 0.9067875750883038, 0, 0.050373788903041165], [0.0, 0.2631578947368421, 0.9067875750883038, 0, 0.05506349753514613], [0.0, 0.15789473684210525, 1.0, 0, 0.044750658730068775], [0.0, 0.6842105263157895, 0.6813046142384115, 0, 0.10561237738348168], [0.0, 0.42105263157894735, 0.6813046142384115, 0, 0.12504128044956234], [0.0, 0.3157894736842105, 0.7851917941024255, 0, 0.08739770758147467], [0.0, 0.42105263157894735, 0.8104475637176851, 0, 0.16114041582366848], [0.0, 1.0, 0.6813046142384115, 0, 0.09811808949646254], [0.0, 0.631578947368421, 0.6813046142384115, 0, 0.11614228731094803], [0.0, 0.5789473684210527, 1.0, 0, 0.061122352540404094], [0.0, 0.631578947368421, 1.0, 0, 0.02175502290631429], [0.0, 0.2631578947368421, 0.7893538079253951, 0, 0.08655450696226076], [0.0, 0.631578947368421, 0.712002162953373, 0, 0.1404638185102031], [0.0, 0.15789473684210525, 0.7851917941024255, 0, 0.09124074484075409], [0.0, 0.3684210526315789, 0.7851917941024255, 0, 0.10019451905627437], [0.0, 0.3157894736842105, 0.9067875750883038, 0, 0.02203110580197674], [0.0, 0.3684210526315789, 0.9067875750883038, 0, 0.023446618230811397], [0.0, 0.10526315789473684, 0.7851917941024255, 0, 0.06220658690668744], [0.0, 0.21052631578947367, 1.0, 0, 0.06562034809161545], [0.25, 0.2631578947368421, 0.9270664157891029, 0, 0.09722459242799925], [0.0, 0.10526315789473684, 1.0, 0, 0.02086418180020698], [0.25, 0.2631578947368421, 0.824272302291662, 0, 0.12151260243966046], [0.25, 0.2631578947368421, 0.6813046142384115, 0, 0.06463228592372475], [0.0, 0.3157894736842105, 1.0, 0, 0.006147033682497226], [0.25, 0.7368421052631579, 1.0, 0, 0.06967850846108996], [0.0, 0.15789473684210525, 0.9067875750883038, 0, 0.028716131544437615], [0.0, 0.47368421052631576, 1.0, 0, 0.06029494380055107], [0.0, 0.21052631578947367, 0.7851917941024255, 0, 0.08982431698610327], [0.0, 0.15789473684210525, 1.0, 0, 0.0], [0.0, 0.2631578947368421, 0.7474398822075096, 0, 0.104531481145718], [0.25, 0.42105263157894735, 1.0, 0, 0.09015884636490967], [0.0, 0.10526315789473684, 1.0, 0, 0.07207522843450107], [0.0, 0.10526315789473684, 0.9389777453063762, 0, 0.05304556214141411], [0.5, 0.3157894736842105, 0.6542274572958134, 0, 0.1276769595272876], [0.0, 0.3157894736842105, 0.7851917941024255, 0, 0.15232481915728807], [0.0, 0.21052631578947367, 1.0, 0, 0.009571731884179967], [0.25, 0.47368421052631576, 0.712002162953373, 0, 0.07529171519182158], [0.25, 0.3157894736842105, 0.6542274572958134, 0, 0.10185679943042204], [0.25, 0.2631578947368421, 0.9067875750883038, 0, 0.07894138249541703], [0.25, 0.5789473684210527, 0.8891189328545094, 0, 0.25075187836096524], [0.0, 0.15789473684210525, 0.8406523071192057, 0, 0.060280509984318144], [0.0, 0.15789473684210525, 1.0, 0, 0.013916811721337453], [0.0, 0.7894736842105263, 0.8406523071192057, 0, 0.09379217403212543], [0.0, 0.15789473684210525, 0.9067875750883038, 0, 0.028293687179213652], [0.0, 0.10526315789473684, 1.0, 0, 0.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.Proper Noun\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "sentence_words = [word_tokenize(sentence) for sentence in preprocessed_sentences]\n",
        "\n",
        "for i in range(len(sentence_words)):\n",
        "    #POS-Part of speech Tagging\n",
        "    pos_tags = nltk.pos_tag(sentence_words[i])\n",
        "\n",
        "    proper_noun_count = sum(1 for word, pos in pos_tags if pos == 'NNP')\n",
        "\n",
        "    sentence_length = len(words)\n",
        "    feature_score = proper_noun_count / sentence_length if sentence_length > 0 else 0\n",
        "    feature_vector[i].append(feature_score)\n",
        "\n",
        "print(feature_vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpI9uWcPOGuf",
        "outputId": "543ce99e-faf7-48a1-a155-3530f31395f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0, 0.10526315789473684, 1.0, 1.0, 0.0, 0.0], [0.0, 0.10526315789473684, 1.0, 0.8, 0.02086418180020698, 0.0], [1.0, 0.5789473684210527, 0.6574114712971478, 0.6, 0.22720867015172186, 0.0], [0.5, 0.7894736842105263, 0.6813046142384115, 0.4, 0.08956937112594304, 0.0], [0.25, 0.9473684210526315, 1.0, 0.2, 0.07124723978793039, 0.0], [0.5, 0.2631578947368421, 0.8891189328545094, 0, 0.18309205162308462, 0.0], [0.25, 0.21052631578947367, 0.9525878668001048, 0, 0.16839073908180144, 0.0], [0.0, 0.42105263157894735, 0.7474398822075096, 0, 0.1002796667282229, 0.0], [0.0, 0.21052631578947367, 1.0, 0, 0.012720739995361378, 0.0], [0.0, 0.3684210526315789, 0.7893538079253951, 0, 0.017245274228144283, 0.0], [0.25, 0.8947368421052632, 1.0, 0, 0.17861757764373995, 0.0], [0.0, 0.5263157894736842, 0.9067875750883038, 0, 0.050373788903041165, 0.0], [0.0, 0.2631578947368421, 0.9067875750883038, 0, 0.05506349753514613, 0.0], [0.0, 0.15789473684210525, 1.0, 0, 0.044750658730068775, 0.0], [0.0, 0.6842105263157895, 0.6813046142384115, 0, 0.10561237738348168, 0.0], [0.0, 0.42105263157894735, 0.6813046142384115, 0, 0.12504128044956234, 0.0], [0.0, 0.3157894736842105, 0.7851917941024255, 0, 0.08739770758147467, 0.0], [0.0, 0.42105263157894735, 0.8104475637176851, 0, 0.16114041582366848, 0.0], [0.0, 1.0, 0.6813046142384115, 0, 0.09811808949646254, 0.0], [0.0, 0.631578947368421, 0.6813046142384115, 0, 0.11614228731094803, 0.0], [0.0, 0.5789473684210527, 1.0, 0, 0.061122352540404094, 0.0], [0.0, 0.631578947368421, 1.0, 0, 0.02175502290631429, 0.0], [0.0, 0.2631578947368421, 0.7893538079253951, 0, 0.08655450696226076, 0.0], [0.0, 0.631578947368421, 0.712002162953373, 0, 0.1404638185102031, 0.0], [0.0, 0.15789473684210525, 0.7851917941024255, 0, 0.09124074484075409, 0.0], [0.0, 0.3684210526315789, 0.7851917941024255, 0, 0.10019451905627437, 0.0], [0.0, 0.3157894736842105, 0.9067875750883038, 0, 0.02203110580197674, 0.0], [0.0, 0.3684210526315789, 0.9067875750883038, 0, 0.023446618230811397, 0.0], [0.0, 0.10526315789473684, 0.7851917941024255, 0, 0.06220658690668744, 0.0], [0.0, 0.21052631578947367, 1.0, 0, 0.06562034809161545, 0.0], [0.25, 0.2631578947368421, 0.9270664157891029, 0, 0.09722459242799925, 0.0], [0.0, 0.10526315789473684, 1.0, 0, 0.02086418180020698, 0.0], [0.25, 0.2631578947368421, 0.824272302291662, 0, 0.12151260243966046, 0.0], [0.25, 0.2631578947368421, 0.6813046142384115, 0, 0.06463228592372475, 0.0], [0.0, 0.3157894736842105, 1.0, 0, 0.006147033682497226, 0.0], [0.25, 0.7368421052631579, 1.0, 0, 0.06967850846108996, 0.0], [0.0, 0.15789473684210525, 0.9067875750883038, 0, 0.028716131544437615, 0.0], [0.0, 0.47368421052631576, 1.0, 0, 0.06029494380055107, 0.0], [0.0, 0.21052631578947367, 0.7851917941024255, 0, 0.08982431698610327, 0.0], [0.0, 0.15789473684210525, 1.0, 0, 0.0, 0.0], [0.0, 0.2631578947368421, 0.7474398822075096, 0, 0.104531481145718, 0.0], [0.25, 0.42105263157894735, 1.0, 0, 0.09015884636490967, 0.0], [0.0, 0.10526315789473684, 1.0, 0, 0.07207522843450107, 0.0], [0.0, 0.10526315789473684, 0.9389777453063762, 0, 0.05304556214141411, 0.0], [0.5, 0.3157894736842105, 0.6542274572958134, 0, 0.1276769595272876, 0.0], [0.0, 0.3157894736842105, 0.7851917941024255, 0, 0.15232481915728807, 0.0], [0.0, 0.21052631578947367, 1.0, 0, 0.009571731884179967, 0.0], [0.25, 0.47368421052631576, 0.712002162953373, 0, 0.07529171519182158, 0.0], [0.25, 0.3157894736842105, 0.6542274572958134, 0, 0.10185679943042204, 0.0], [0.25, 0.2631578947368421, 0.9067875750883038, 0, 0.07894138249541703, 0.0], [0.25, 0.5789473684210527, 0.8891189328545094, 0, 0.25075187836096524, 0.0], [0.0, 0.15789473684210525, 0.8406523071192057, 0, 0.060280509984318144, 0.0], [0.0, 0.15789473684210525, 1.0, 0, 0.013916811721337453, 0.0], [0.0, 0.7894736842105263, 0.8406523071192057, 0, 0.09379217403212543, 0.0], [0.0, 0.15789473684210525, 0.9067875750883038, 0, 0.028293687179213652, 0.0], [0.0, 0.10526315789473684, 1.0, 0, 0.0, 0.0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.Thematic Word\n",
        "from collections import Counter\n",
        "all_words = [word for words in sentence_words for word in words]\n",
        "\n",
        "# top 10 most frequent content words\n",
        "word_frequencies = Counter(all_words)\n",
        "most_common_words = [word for word, freq in word_frequencies.most_common(10)]\n",
        "\n",
        "for i in range(len(sentence_words)):\n",
        "    thematic_word_count = sum(1 for word in sentence_words[i] if word in most_common_words)\n",
        "\n",
        "    sentence_length = len(words)\n",
        "    feature_score = thematic_word_count / sentence_length if sentence_length > 0 else 0\n",
        "    feature_vector[i].append(feature_score)\n",
        "\n",
        "print(feature_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OA1Fnco-PiZs",
        "outputId": "dec56eea-02c1-4a96-9c4d-fe6dbca419e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0, 0.10526315789473684, 1.0, 1.0, 0.0, 0.0, 0.0], [0.0, 0.10526315789473684, 1.0, 0.8, 0.02086418180020698, 0.0, 0.0], [1.0, 0.5789473684210527, 0.6574114712971478, 0.6, 0.22720867015172186, 0.0, 3.0], [0.5, 0.7894736842105263, 0.6813046142384115, 0.4, 0.08956937112594304, 0.0, 2.0], [0.25, 0.9473684210526315, 1.0, 0.2, 0.07124723978793039, 0.0, 1.0], [0.5, 0.2631578947368421, 0.8891189328545094, 0, 0.18309205162308462, 0.0, 1.5], [0.25, 0.21052631578947367, 0.9525878668001048, 0, 0.16839073908180144, 0.0, 1.0], [0.0, 0.42105263157894735, 0.7474398822075096, 0, 0.1002796667282229, 0.0, 0.0], [0.0, 0.21052631578947367, 1.0, 0, 0.012720739995361378, 0.0, 0.0], [0.0, 0.3684210526315789, 0.7893538079253951, 0, 0.017245274228144283, 0.0, 0.0], [0.25, 0.8947368421052632, 1.0, 0, 0.17861757764373995, 0.0, 4.0], [0.0, 0.5263157894736842, 0.9067875750883038, 0, 0.050373788903041165, 0.0, 0.5], [0.0, 0.2631578947368421, 0.9067875750883038, 0, 0.05506349753514613, 0.0, 0.0], [0.0, 0.15789473684210525, 1.0, 0, 0.044750658730068775, 0.0, 0.5], [0.0, 0.6842105263157895, 0.6813046142384115, 0, 0.10561237738348168, 0.0, 1.5], [0.0, 0.42105263157894735, 0.6813046142384115, 0, 0.12504128044956234, 0.0, 1.0], [0.0, 0.3157894736842105, 0.7851917941024255, 0, 0.08739770758147467, 0.0, 0.0], [0.0, 0.42105263157894735, 0.8104475637176851, 0, 0.16114041582366848, 0.0, 2.0], [0.0, 1.0, 0.6813046142384115, 0, 0.09811808949646254, 0.0, 2.0], [0.0, 0.631578947368421, 0.6813046142384115, 0, 0.11614228731094803, 0.0, 2.5], [0.0, 0.5789473684210527, 1.0, 0, 0.061122352540404094, 0.0, 1.0], [0.0, 0.631578947368421, 1.0, 0, 0.02175502290631429, 0.0, 0.0], [0.0, 0.2631578947368421, 0.7893538079253951, 0, 0.08655450696226076, 0.0, 0.5], [0.0, 0.631578947368421, 0.712002162953373, 0, 0.1404638185102031, 0.0, 2.0], [0.0, 0.15789473684210525, 0.7851917941024255, 0, 0.09124074484075409, 0.0, 0.5], [0.0, 0.3684210526315789, 0.7851917941024255, 0, 0.10019451905627437, 0.0, 0.0], [0.0, 0.3157894736842105, 0.9067875750883038, 0, 0.02203110580197674, 0.0, 0.0], [0.0, 0.3684210526315789, 0.9067875750883038, 0, 0.023446618230811397, 0.0, 0.0], [0.0, 0.10526315789473684, 0.7851917941024255, 0, 0.06220658690668744, 0.0, 0.0], [0.0, 0.21052631578947367, 1.0, 0, 0.06562034809161545, 0.0, 0.0], [0.25, 0.2631578947368421, 0.9270664157891029, 0, 0.09722459242799925, 0.0, 1.0], [0.0, 0.10526315789473684, 1.0, 0, 0.02086418180020698, 0.0, 0.0], [0.25, 0.2631578947368421, 0.824272302291662, 0, 0.12151260243966046, 0.0, 0.5], [0.25, 0.2631578947368421, 0.6813046142384115, 0, 0.06463228592372475, 0.0, 1.0], [0.0, 0.3157894736842105, 1.0, 0, 0.006147033682497226, 0.0, 0.0], [0.25, 0.7368421052631579, 1.0, 0, 0.06967850846108996, 0.0, 1.0], [0.0, 0.15789473684210525, 0.9067875750883038, 0, 0.028716131544437615, 0.0, 0.0], [0.0, 0.47368421052631576, 1.0, 0, 0.06029494380055107, 0.0, 0.0], [0.0, 0.21052631578947367, 0.7851917941024255, 0, 0.08982431698610327, 0.0, 0.0], [0.0, 0.15789473684210525, 1.0, 0, 0.0, 0.0, 0.0], [0.0, 0.2631578947368421, 0.7474398822075096, 0, 0.104531481145718, 0.0, 0.5], [0.25, 0.42105263157894735, 1.0, 0, 0.09015884636490967, 0.0, 1.0], [0.0, 0.10526315789473684, 1.0, 0, 0.07207522843450107, 0.0, 0.0], [0.0, 0.10526315789473684, 0.9389777453063762, 0, 0.05304556214141411, 0.0, 0.0], [0.5, 0.3157894736842105, 0.6542274572958134, 0, 0.1276769595272876, 0.0, 1.5], [0.0, 0.3157894736842105, 0.7851917941024255, 0, 0.15232481915728807, 0.0, 1.5], [0.0, 0.21052631578947367, 1.0, 0, 0.009571731884179967, 0.0, 0.0], [0.25, 0.47368421052631576, 0.712002162953373, 0, 0.07529171519182158, 0.0, 1.5], [0.25, 0.3157894736842105, 0.6542274572958134, 0, 0.10185679943042204, 0.0, 1.0], [0.25, 0.2631578947368421, 0.9067875750883038, 0, 0.07894138249541703, 0.0, 1.0], [0.25, 0.5789473684210527, 0.8891189328545094, 0, 0.25075187836096524, 0.0, 2.5], [0.0, 0.15789473684210525, 0.8406523071192057, 0, 0.060280509984318144, 0.0, 0.5], [0.0, 0.15789473684210525, 1.0, 0, 0.013916811721337453, 0.0, 0.0], [0.0, 0.7894736842105263, 0.8406523071192057, 0, 0.09379217403212543, 0.0, 1.5], [0.0, 0.15789473684210525, 0.9067875750883038, 0, 0.028293687179213652, 0.0, 0.0], [0.0, 0.10526315789473684, 1.0, 0, 0.0, 0.0, 0.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.Numerical Data\n",
        "for i in range(len(sentence_words)):\n",
        "    numerical_data_count = sum(1 for word in sentence_words[i] if re.match(r'^[0-9]+$', word))\n",
        "\n",
        "    sentence_length = len(sentence_words[i])\n",
        "    feature_score = numerical_data_count / sentence_length if sentence_length > 0 else 0\n",
        "    feature_vector[i].append(feature_score)\n",
        "\n",
        "print(feature_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GB6WKCZPjMx",
        "outputId": "14218169-4732-4c49-8adb-91d05a8eed32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0, 0.10526315789473684, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.10526315789473684, 1.0, 0.8, 0.02086418180020698, 0.0, 0.0, 0.0], [1.0, 0.5789473684210527, 0.6574114712971478, 0.6, 0.22720867015172186, 0.0, 3.0, 0.0], [0.5, 0.7894736842105263, 0.6813046142384115, 0.4, 0.08956937112594304, 0.0, 2.0, 0.0], [0.25, 0.9473684210526315, 1.0, 0.2, 0.07124723978793039, 0.0, 1.0, 0.0], [0.5, 0.2631578947368421, 0.8891189328545094, 0, 0.18309205162308462, 0.0, 1.5, 0.0], [0.25, 0.21052631578947367, 0.9525878668001048, 0, 0.16839073908180144, 0.0, 1.0, 0.0], [0.0, 0.42105263157894735, 0.7474398822075096, 0, 0.1002796667282229, 0.0, 0.0, 0.0], [0.0, 0.21052631578947367, 1.0, 0, 0.012720739995361378, 0.0, 0.0, 0.0], [0.0, 0.3684210526315789, 0.7893538079253951, 0, 0.017245274228144283, 0.0, 0.0, 0.0], [0.25, 0.8947368421052632, 1.0, 0, 0.17861757764373995, 0.0, 4.0, 0.0], [0.0, 0.5263157894736842, 0.9067875750883038, 0, 0.050373788903041165, 0.0, 0.5, 0.0], [0.0, 0.2631578947368421, 0.9067875750883038, 0, 0.05506349753514613, 0.0, 0.0, 0.0], [0.0, 0.15789473684210525, 1.0, 0, 0.044750658730068775, 0.0, 0.5, 0.0], [0.0, 0.6842105263157895, 0.6813046142384115, 0, 0.10561237738348168, 0.0, 1.5, 0.0], [0.0, 0.42105263157894735, 0.6813046142384115, 0, 0.12504128044956234, 0.0, 1.0, 0.0], [0.0, 0.3157894736842105, 0.7851917941024255, 0, 0.08739770758147467, 0.0, 0.0, 0.0], [0.0, 0.42105263157894735, 0.8104475637176851, 0, 0.16114041582366848, 0.0, 2.0, 0.0], [0.0, 1.0, 0.6813046142384115, 0, 0.09811808949646254, 0.0, 2.0, 0.0], [0.0, 0.631578947368421, 0.6813046142384115, 0, 0.11614228731094803, 0.0, 2.5, 0.0], [0.0, 0.5789473684210527, 1.0, 0, 0.061122352540404094, 0.0, 1.0, 0.0], [0.0, 0.631578947368421, 1.0, 0, 0.02175502290631429, 0.0, 0.0, 0.0], [0.0, 0.2631578947368421, 0.7893538079253951, 0, 0.08655450696226076, 0.0, 0.5, 0.0], [0.0, 0.631578947368421, 0.712002162953373, 0, 0.1404638185102031, 0.0, 2.0, 0.0], [0.0, 0.15789473684210525, 0.7851917941024255, 0, 0.09124074484075409, 0.0, 0.5, 0.0], [0.0, 0.3684210526315789, 0.7851917941024255, 0, 0.10019451905627437, 0.0, 0.0, 0.0], [0.0, 0.3157894736842105, 0.9067875750883038, 0, 0.02203110580197674, 0.0, 0.0, 0.0], [0.0, 0.3684210526315789, 0.9067875750883038, 0, 0.023446618230811397, 0.0, 0.0, 0.0], [0.0, 0.10526315789473684, 0.7851917941024255, 0, 0.06220658690668744, 0.0, 0.0, 0.0], [0.0, 0.21052631578947367, 1.0, 0, 0.06562034809161545, 0.0, 0.0, 0.0], [0.25, 0.2631578947368421, 0.9270664157891029, 0, 0.09722459242799925, 0.0, 1.0, 0.0], [0.0, 0.10526315789473684, 1.0, 0, 0.02086418180020698, 0.0, 0.0, 0.0], [0.25, 0.2631578947368421, 0.824272302291662, 0, 0.12151260243966046, 0.0, 0.5, 0.0], [0.25, 0.2631578947368421, 0.6813046142384115, 0, 0.06463228592372475, 0.0, 1.0, 0.0], [0.0, 0.3157894736842105, 1.0, 0, 0.006147033682497226, 0.0, 0.0, 0.0], [0.25, 0.7368421052631579, 1.0, 0, 0.06967850846108996, 0.0, 1.0, 0.0], [0.0, 0.15789473684210525, 0.9067875750883038, 0, 0.028716131544437615, 0.0, 0.0, 0.0], [0.0, 0.47368421052631576, 1.0, 0, 0.06029494380055107, 0.0, 0.0, 0.0], [0.0, 0.21052631578947367, 0.7851917941024255, 0, 0.08982431698610327, 0.0, 0.0, 0.0], [0.0, 0.15789473684210525, 1.0, 0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.2631578947368421, 0.7474398822075096, 0, 0.104531481145718, 0.0, 0.5, 0.0], [0.25, 0.42105263157894735, 1.0, 0, 0.09015884636490967, 0.0, 1.0, 0.0], [0.0, 0.10526315789473684, 1.0, 0, 0.07207522843450107, 0.0, 0.0, 0.0], [0.0, 0.10526315789473684, 0.9389777453063762, 0, 0.05304556214141411, 0.0, 0.0, 0.0], [0.5, 0.3157894736842105, 0.6542274572958134, 0, 0.1276769595272876, 0.0, 1.5, 0.0], [0.0, 0.3157894736842105, 0.7851917941024255, 0, 0.15232481915728807, 0.0, 1.5, 0.0], [0.0, 0.21052631578947367, 1.0, 0, 0.009571731884179967, 0.0, 0.0, 0.0], [0.25, 0.47368421052631576, 0.712002162953373, 0, 0.07529171519182158, 0.0, 1.5, 0.0], [0.25, 0.3157894736842105, 0.6542274572958134, 0, 0.10185679943042204, 0.0, 1.0, 0.0], [0.25, 0.2631578947368421, 0.9067875750883038, 0, 0.07894138249541703, 0.0, 1.0, 0.0], [0.25, 0.5789473684210527, 0.8891189328545094, 0, 0.25075187836096524, 0.0, 2.5, 0.0], [0.0, 0.15789473684210525, 0.8406523071192057, 0, 0.060280509984318144, 0.0, 0.5, 0.0], [0.0, 0.15789473684210525, 1.0, 0, 0.013916811721337453, 0.0, 0.0, 0.0], [0.0, 0.7894736842105263, 0.8406523071192057, 0, 0.09379217403212543, 0.0, 1.5, 0.0], [0.0, 0.15789473684210525, 0.9067875750883038, 0, 0.028293687179213652, 0.0, 0.0, 0.0], [0.0, 0.10526315789473684, 1.0, 0, 0.0, 0.0, 0.0, 0.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentance Scoring**\n"
      ],
      "metadata": {
        "id": "2NKo8wQKQeW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentance_score=[]\n",
        "for i in feature_vector:\n",
        "  sentance_score.append(sum(i))\n",
        "\n",
        "for i, sc in enumerate(sentance_score):\n",
        "    print(f\"Sentence {i + 1} - Score : \",sc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMfdRBIuQbzj",
        "outputId": "97440ce8-c13c-4e03-8ca5-f91a5e01acf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1 - Score :  2.1052631578947367\n",
            "Sentence 2 - Score :  1.9261273396949437\n",
            "Sentence 3 - Score :  6.063567509869922\n",
            "Sentence 4 - Score :  4.4603476695748805\n",
            "Sentence 5 - Score :  3.468615660840562\n",
            "Sentence 6 - Score :  3.3353688792144363\n",
            "Sentence 7 - Score :  2.58150492167138\n",
            "Sentence 8 - Score :  1.2687721805146799\n",
            "Sentence 9 - Score :  1.223247055784835\n",
            "Sentence 10 - Score :  1.1750201347851184\n",
            "Sentence 11 - Score :  6.323354419749004\n",
            "Sentence 12 - Score :  1.983477153465029\n",
            "Sentence 13 - Score :  1.225008967360292\n",
            "Sentence 14 - Score :  1.702645395572174\n",
            "Sentence 15 - Score :  2.9711275179376826\n",
            "Sentence 16 - Score :  2.2273985262669216\n",
            "Sentence 17 - Score :  1.1883789753681107\n",
            "Sentence 18 - Score :  3.392640611120301\n",
            "Sentence 19 - Score :  3.779422703734874\n",
            "Sentence 20 - Score :  3.9290258489177807\n",
            "Sentence 21 - Score :  2.6400697209614568\n",
            "Sentence 22 - Score :  1.6533339702747354\n",
            "Sentence 23 - Score :  1.6390662096244981\n",
            "Sentence 24 - Score :  3.484044928831997\n",
            "Sentence 25 - Score :  1.5343272757852848\n",
            "Sentence 26 - Score :  1.2538073657902786\n",
            "Sentence 27 - Score :  1.244608154574491\n",
            "Sentence 28 - Score :  1.298655245950694\n",
            "Sentence 29 - Score :  0.9526615389038497\n",
            "Sentence 30 - Score :  1.2761466638810892\n",
            "Sentence 31 - Score :  2.537448902953944\n",
            "Sentence 32 - Score :  1.1261273396949436\n",
            "Sentence 33 - Score :  1.9589427994681645\n",
            "Sentence 34 - Score :  2.2590947948989784\n",
            "Sentence 35 - Score :  1.3219365073667078\n",
            "Sentence 36 - Score :  3.0565206137242478\n",
            "Sentence 37 - Score :  1.0933984434748465\n",
            "Sentence 38 - Score :  1.5339791543268668\n",
            "Sentence 39 - Score :  1.0855424268780025\n",
            "Sentence 40 - Score :  1.1578947368421053\n",
            "Sentence 41 - Score :  1.6151292580900698\n",
            "Sentence 42 - Score :  2.7612114779438572\n",
            "Sentence 43 - Score :  1.1773383863292377\n",
            "Sentence 44 - Score :  1.0972864653425272\n",
            "Sentence 45 - Score :  3.0976938905073115\n",
            "Sentence 46 - Score :  2.7533060869439243\n",
            "Sentence 47 - Score :  1.2200980476736536\n",
            "Sentence 48 - Score :  3.0109780886715103\n",
            "Sentence 49 - Score :  2.321873730410446\n",
            "Sentence 50 - Score :  2.4988868523205627\n",
            "Sentence 51 - Score :  4.468818179636528\n",
            "Sentence 52 - Score :  1.558827553945629\n",
            "Sentence 53 - Score :  1.1718115485634428\n",
            "Sentence 54 - Score :  3.223918165361858\n",
            "Sentence 55 - Score :  1.0929759991096226\n",
            "Sentence 56 - Score :  1.1052631578947367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict={}\n",
        "for i in range(len(before_proprocessed)):\n",
        "  dict[sentance_score[i]]=before_proprocessed[i]\n",
        "dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1uCjFGQJpmF",
        "outputId": "534322d6-63a8-4197-c706-9d647952b5a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{2.1052631578947367: 'Hello, this is AJ.',\n",
              " 1.9261273396949437: 'I have some very exciting news.',\n",
              " 6.063567509869922: 'We are having our seminar in Bangkok, our first effortless English breakthrough seminar in Bangkok, Thailand on January 25th.',\n",
              " 4.4603476695748805: \"Now I'm telling everybody about this, even if you don't live in Bangkok, because we are developing these seminars so that in the future we can visit many cities around the world.\",\n",
              " 3.468615660840562: \"For example, this summer we will definitely be coming to Japan and we'll be doing effortless English breakthrough seminars in Japan, probably in the Kansai area in Kyoto, Osaka, possibly Kobe.\",\n",
              " 3.3353688792144363: \"But right now our first seminars we're doing in Bangkok, Thailand.\",\n",
              " 2.58150492167138: 'And our very first seminar is January 25th.',\n",
              " 1.2687721805146799: \"So this January 25th starting at 1045 until 4 p.m. and that's on a Sunday, January 25th.\",\n",
              " 1.223247055784835: 'It will be at the Royal Hotel, which is at number 2, Ratchadamnurn.',\n",
              " 1.1750201347851184: \"And that's across from Sanam Luang, across from the big field.\",\n",
              " 6.323354419749004: \"But anyway, at these seminars what I'm going to do is number one, I'm going to talk about the effortless English system and especially about some new things that we are focusing on with our system.\",\n",
              " 1.983477153465029: 'So you know about the many stories, you know about using, you know, listen first and focusing on input.',\n",
              " 1.225008967360292: \"And I'll talk some about that too because some people might not know about it.\",\n",
              " 1.702645395572174: 'There may be some new people there.',\n",
              " 2.9711275179376826: \"But I'm also going to talk about some of the new things that we are adding to our system in order to make the system even more powerful.\",\n",
              " 2.2273985262669216: 'And these new elements are especially powerful live in person with me because we can have a lot of interaction.',\n",
              " 1.1883789753681107: \"There can be a lot more energy when we're all in the same room together, learning together.\",\n",
              " 3.392640611120301: \"So I'm going to talk about these new elements of the effortless English system.\",\n",
              " 3.779422703734874: \"And I'm going to talk about the general elements of the effortless English system, how to learn English most quickly, most easily, most effectively, how to speak easily, fast and fluently without thinking.\",\n",
              " 3.9290258489177807: \"And then I'm going to actually do some demonstration lessons, including a mini story lesson and a point of view lesson.\",\n",
              " 2.6400697209614568: 'And remember the point of view lessons, those are the lessons where I teach grammar in a natural way, in an intuitive way.',\n",
              " 1.6533339702747354: 'So you, the student, you never think about grammar rules, but you still learn the grammar, but you learn it like a native speaker.',\n",
              " 1.6390662096244981: \"That's the power of those point of view lessons.\",\n",
              " 3.484044928831997: \"So we'll experience them live, the mini story lessons and the point of view lessons, which are the most powerful lesson types that we have.\",\n",
              " 1.5343272757852848: \"And we'll do it live in person.\",\n",
              " 1.2538073657902786: \"It's a lot more fun when we're all together in a room because we create the stories together.\",\n",
              " 1.244608154574491: \"You're not just listening to me tell a story and ask questions.\",\n",
              " 1.298655245950694: \"You're actually shouting out answers and I change the story with your answers.\",\n",
              " 0.9526615389038497: \"It's a lot of fun.\",\n",
              " 1.2761466638810892: \"There's a lot of energy in the room.\",\n",
              " 2.537448902953944: 'So if you can come to one of our seminars, definitely come.',\n",
              " 1.1261273396949436: 'More good news.',\n",
              " 1.9589427994681645: \"Right now, the seminars are free because right now we're just developing them.\",\n",
              " 2.2590947948989784: \"I'm testing different parts of the seminar.\",\n",
              " 1.3219365073667078: 'I want to create something that is amazing, that is remarkable, that is outstanding.',\n",
              " 3.0565206137242478: 'Now, eventually, we will develop a longer seminar that will be possibly two, three, maybe four days, a very intensive English learning experience.',\n",
              " 1.0933984434748465: 'At that time, we will charge money.',\n",
              " 1.5339791543268668: \"You'll have to pay because we'll need to get a bigger conference room at a hotel.\",\n",
              " 1.0855424268780025: \"We'll have to pay a lot more money.\",\n",
              " 1.1578947368421053: \"So it'll become more expensive.\",\n",
              " 1.6151292580900698: \"But right now is a great opportunity because it's just a one day.\",\n",
              " 2.7612114779438572: \"Really, it's a half day seminar just to get a quick breakthrough with your English.\",\n",
              " 1.1773383863292377: \"And it's free right now.\",\n",
              " 1.0972864653425272: \"So that's great.\",\n",
              " 3.0976938905073115: 'So if you live in Bangkok or in Southeast Asia, come to our seminars.',\n",
              " 2.7533060869439243: \"We're going to have the first one January 25th on Sunday.\",\n",
              " 1.2200980476736536: 'Go to our website, effortlessenglishclub.com for more information.',\n",
              " 3.0109780886715103: \"We'll have another one in February and another one in March, also in Bangkok.\",\n",
              " 2.321873730410446: 'So if you live in Thailand or you can get to Bangkok, check our website.',\n",
              " 2.4988868523205627: \"I'll announce the new seminars when we schedule them.\",\n",
              " 4.468818179636528: 'Right now, the first one, January 25th, come to Bangkok and experience effortless English live.',\n",
              " 1.558827553945629: 'Learn with me personally, live.',\n",
              " 1.1718115485634428: 'Okay, I hope to see you there.',\n",
              " 3.223918165361858: 'And have a great day and enjoy your English learning and start this new year with powerful English learning, powerful motivation, powerful goals.',\n",
              " 1.0929759991096226: 'See you next time.',\n",
              " 1.1052631578947367: 'Bye bye.'}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import operator\n",
        "sorted_sentence_dict = {k: v for k, v in sorted(dict.items(), key=lambda item: item[0], reverse=True)}\n",
        "print(sorted_sentence_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKfL7mQHQ6Q0",
        "outputId": "808f0543-a477-4819-d25d-5d9b6d7de6d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{6.323354419749004: \"But anyway, at these seminars what I'm going to do is number one, I'm going to talk about the effortless English system and especially about some new things that we are focusing on with our system.\", 6.063567509869922: 'We are having our seminar in Bangkok, our first effortless English breakthrough seminar in Bangkok, Thailand on January 25th.', 4.468818179636528: 'Right now, the first one, January 25th, come to Bangkok and experience effortless English live.', 4.4603476695748805: \"Now I'm telling everybody about this, even if you don't live in Bangkok, because we are developing these seminars so that in the future we can visit many cities around the world.\", 3.9290258489177807: \"And then I'm going to actually do some demonstration lessons, including a mini story lesson and a point of view lesson.\", 3.779422703734874: \"And I'm going to talk about the general elements of the effortless English system, how to learn English most quickly, most easily, most effectively, how to speak easily, fast and fluently without thinking.\", 3.484044928831997: \"So we'll experience them live, the mini story lessons and the point of view lessons, which are the most powerful lesson types that we have.\", 3.468615660840562: \"For example, this summer we will definitely be coming to Japan and we'll be doing effortless English breakthrough seminars in Japan, probably in the Kansai area in Kyoto, Osaka, possibly Kobe.\", 3.392640611120301: \"So I'm going to talk about these new elements of the effortless English system.\", 3.3353688792144363: \"But right now our first seminars we're doing in Bangkok, Thailand.\", 3.223918165361858: 'And have a great day and enjoy your English learning and start this new year with powerful English learning, powerful motivation, powerful goals.', 3.0976938905073115: 'So if you live in Bangkok or in Southeast Asia, come to our seminars.', 3.0565206137242478: 'Now, eventually, we will develop a longer seminar that will be possibly two, three, maybe four days, a very intensive English learning experience.', 3.0109780886715103: \"We'll have another one in February and another one in March, also in Bangkok.\", 2.9711275179376826: \"But I'm also going to talk about some of the new things that we are adding to our system in order to make the system even more powerful.\", 2.7612114779438572: \"Really, it's a half day seminar just to get a quick breakthrough with your English.\", 2.7533060869439243: \"We're going to have the first one January 25th on Sunday.\", 2.6400697209614568: 'And remember the point of view lessons, those are the lessons where I teach grammar in a natural way, in an intuitive way.', 2.58150492167138: 'And our very first seminar is January 25th.', 2.537448902953944: 'So if you can come to one of our seminars, definitely come.', 2.4988868523205627: \"I'll announce the new seminars when we schedule them.\", 2.321873730410446: 'So if you live in Thailand or you can get to Bangkok, check our website.', 2.2590947948989784: \"I'm testing different parts of the seminar.\", 2.2273985262669216: 'And these new elements are especially powerful live in person with me because we can have a lot of interaction.', 2.1052631578947367: 'Hello, this is AJ.', 1.983477153465029: 'So you know about the many stories, you know about using, you know, listen first and focusing on input.', 1.9589427994681645: \"Right now, the seminars are free because right now we're just developing them.\", 1.9261273396949437: 'I have some very exciting news.', 1.702645395572174: 'There may be some new people there.', 1.6533339702747354: 'So you, the student, you never think about grammar rules, but you still learn the grammar, but you learn it like a native speaker.', 1.6390662096244981: \"That's the power of those point of view lessons.\", 1.6151292580900698: \"But right now is a great opportunity because it's just a one day.\", 1.558827553945629: 'Learn with me personally, live.', 1.5343272757852848: \"And we'll do it live in person.\", 1.5339791543268668: \"You'll have to pay because we'll need to get a bigger conference room at a hotel.\", 1.3219365073667078: 'I want to create something that is amazing, that is remarkable, that is outstanding.', 1.298655245950694: \"You're actually shouting out answers and I change the story with your answers.\", 1.2761466638810892: \"There's a lot of energy in the room.\", 1.2687721805146799: \"So this January 25th starting at 1045 until 4 p.m. and that's on a Sunday, January 25th.\", 1.2538073657902786: \"It's a lot more fun when we're all together in a room because we create the stories together.\", 1.244608154574491: \"You're not just listening to me tell a story and ask questions.\", 1.225008967360292: \"And I'll talk some about that too because some people might not know about it.\", 1.223247055784835: 'It will be at the Royal Hotel, which is at number 2, Ratchadamnurn.', 1.2200980476736536: 'Go to our website, effortlessenglishclub.com for more information.', 1.1883789753681107: \"There can be a lot more energy when we're all in the same room together, learning together.\", 1.1773383863292377: \"And it's free right now.\", 1.1750201347851184: \"And that's across from Sanam Luang, across from the big field.\", 1.1718115485634428: 'Okay, I hope to see you there.', 1.1578947368421053: \"So it'll become more expensive.\", 1.1261273396949436: 'More good news.', 1.1052631578947367: 'Bye bye.', 1.0972864653425272: \"So that's great.\", 1.0933984434748465: 'At that time, we will charge money.', 1.0929759991096226: 'See you next time.', 1.0855424268780025: \"We'll have to pay a lot more money.\", 0.9526615389038497: \"It's a lot of fun.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_sentences = list(sorted_sentence_dict.values())[:10]\n",
        "combined_sentence = \" \".join(selected_sentences)\n",
        "combined_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "ilxvihoyT3cO",
        "outputId": "8ad59056-043e-4f3f-dbca-dd3d2503ec6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"But anyway, at these seminars what I'm going to do is number one, I'm going to talk about the effortless English system and especially about some new things that we are focusing on with our system. We are having our seminar in Bangkok, our first effortless English breakthrough seminar in Bangkok, Thailand on January 25th. Right now, the first one, January 25th, come to Bangkok and experience effortless English live. Now I'm telling everybody about this, even if you don't live in Bangkok, because we are developing these seminars so that in the future we can visit many cities around the world. And then I'm going to actually do some demonstration lessons, including a mini story lesson and a point of view lesson. And I'm going to talk about the general elements of the effortless English system, how to learn English most quickly, most easily, most effectively, how to speak easily, fast and fluently without thinking. So we'll experience them live, the mini story lessons and the point of view lessons, which are the most powerful lesson types that we have. For example, this summer we will definitely be coming to Japan and we'll be doing effortless English breakthrough seminars in Japan, probably in the Kansai area in Kyoto, Osaka, possibly Kobe. So I'm going to talk about these new elements of the effortless English system. But right now our first seminars we're doing in Bangkok, Thailand.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tfidf vectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(preprocessed_sentences)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "tfidf_array = tfidf_matrix.toarray()\n",
        "tdidf_scores = [sum(sentence_tfidf) for sentence_tfidf in tfidf_array]\n",
        "#print(sentence_scores)\n",
        "for i, sentence_tfidf in enumerate(tdidf_scores):\n",
        "    print(f\"Sentence {i + 1} - TF-IDF Values : \",sentence_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL7WPMYmWEAP",
        "outputId": "109ffcd5-9f0f-4c4f-bf83-ce97f96f7933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1 - TF-IDF Values :  1.4142135623730951\n",
            "Sentence 2 - TF-IDF Values :  1.4125268133596947\n",
            "Sentence 3 - TF-IDF Values :  2.9054781407661343\n",
            "Sentence 4 - TF-IDF Values :  3.8160597224342214\n",
            "Sentence 5 - TF-IDF Values :  3.936044712265364\n",
            "Sentence 6 - TF-IDF Values :  2.2166279063327154\n",
            "Sentence 7 - TF-IDF Values :  1.9865086625283657\n",
            "Sentence 8 - TF-IDF Values :  2.376530628521342\n",
            "Sentence 9 - TF-IDF Values :  1.9976145766689293\n",
            "Sentence 10 - TF-IDF Values :  2.312050138866276\n",
            "Sentence 11 - TF-IDF Values :  3.5582905470074784\n",
            "Sentence 12 - TF-IDF Values :  2.4788868576821432\n",
            "Sentence 13 - TF-IDF Values :  2.226826364718689\n",
            "Sentence 14 - TF-IDF Values :  1.71159050073959\n",
            "Sentence 15 - TF-IDF Values :  3.3549016905969364\n",
            "Sentence 16 - TF-IDF Values :  2.800226389513972\n",
            "Sentence 17 - TF-IDF Values :  2.0757114476505008\n",
            "Sentence 18 - TF-IDF Values :  2.817972023910201\n",
            "Sentence 19 - TF-IDF Values :  3.9461029918224293\n",
            "Sentence 20 - TF-IDF Values :  2.850764896464866\n",
            "Sentence 21 - TF-IDF Values :  2.839695422542164\n",
            "Sentence 22 - TF-IDF Values :  3.0595043070826504\n",
            "Sentence 23 - TF-IDF Values :  2.2246747625864973\n",
            "Sentence 24 - TF-IDF Values :  2.826037967953922\n",
            "Sentence 25 - TF-IDF Values :  1.7147038167331208\n",
            "Sentence 26 - TF-IDF Values :  2.2785029774139285\n",
            "Sentence 27 - TF-IDF Values :  2.433948201349984\n",
            "Sentence 28 - TF-IDF Values :  2.2856032897762777\n",
            "Sentence 29 - TF-IDF Values :  1.4040853699410754\n",
            "Sentence 30 - TF-IDF Values :  1.9835280862940992\n",
            "Sentence 31 - TF-IDF Values :  1.8240384867704147\n",
            "Sentence 32 - TF-IDF Values :  1.4125268133596947\n",
            "Sentence 33 - TF-IDF Values :  1.8874875601793666\n",
            "Sentence 34 - TF-IDF Values :  2.1807147095674186\n",
            "Sentence 35 - TF-IDF Values :  2.4479661980602003\n",
            "Sentence 36 - TF-IDF Values :  3.6939779746814567\n",
            "Sentence 37 - TF-IDF Values :  1.7301528918667914\n",
            "Sentence 38 - TF-IDF Values :  2.981727455094678\n",
            "Sentence 39 - TF-IDF Values :  1.9856765729003136\n",
            "Sentence 40 - TF-IDF Values :  1.7320508075688776\n",
            "Sentence 41 - TF-IDF Values :  2.219319996329345\n",
            "Sentence 42 - TF-IDF Values :  2.777330404469449\n",
            "Sentence 43 - TF-IDF Values :  1.4076976306225877\n",
            "Sentence 44 - TF-IDF Values :  1.4135137310269565\n",
            "Sentence 45 - TF-IDF Values :  2.3932177292101104\n",
            "Sentence 46 - TF-IDF Values :  2.4393142773631107\n",
            "Sentence 47 - TF-IDF Values :  1.998294414676501\n",
            "Sentence 48 - TF-IDF Values :  2.4647114732460937\n",
            "Sentence 49 - TF-IDF Values :  2.423389196780132\n",
            "Sentence 50 - TF-IDF Values :  2.184834564975354\n",
            "Sentence 51 - TF-IDF Values :  3.3040973136245366\n",
            "Sentence 52 - TF-IDF Values :  1.707576588213112\n",
            "Sentence 53 - TF-IDF Values :  1.7302724773813032\n",
            "Sentence 54 - TF-IDF Values :  3.077660058194448\n",
            "Sentence 55 - TF-IDF Values :  1.7301528918667914\n",
            "Sentence 56 - TF-IDF Values :  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict1={}\n",
        "for i in range(len(before_proprocessed)):\n",
        "  dict1[tdidf_scores[i]]=before_proprocessed[i]\n",
        "\n",
        "t = {k: v for k, v in sorted(dict1.items(), key=lambda item: item[0], reverse=True)}\n",
        "print(t)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3gRrlJGWqka",
        "outputId": "66689a67-75f9-4fbb-f11f-5629973995bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{3.9461029918224293: \"And I'm going to talk about the general elements of the effortless English system, how to learn English most quickly, most easily, most effectively, how to speak easily, fast and fluently without thinking.\", 3.936044712265364: \"For example, this summer we will definitely be coming to Japan and we'll be doing effortless English breakthrough seminars in Japan, probably in the Kansai area in Kyoto, Osaka, possibly Kobe.\", 3.8160597224342214: \"Now I'm telling everybody about this, even if you don't live in Bangkok, because we are developing these seminars so that in the future we can visit many cities around the world.\", 3.6939779746814567: 'Now, eventually, we will develop a longer seminar that will be possibly two, three, maybe four days, a very intensive English learning experience.', 3.5582905470074784: \"But anyway, at these seminars what I'm going to do is number one, I'm going to talk about the effortless English system and especially about some new things that we are focusing on with our system.\", 3.3549016905969364: \"But I'm also going to talk about some of the new things that we are adding to our system in order to make the system even more powerful.\", 3.3040973136245366: 'Right now, the first one, January 25th, come to Bangkok and experience effortless English live.', 3.077660058194448: 'And have a great day and enjoy your English learning and start this new year with powerful English learning, powerful motivation, powerful goals.', 3.0595043070826504: 'So you, the student, you never think about grammar rules, but you still learn the grammar, but you learn it like a native speaker.', 2.981727455094678: \"You'll have to pay because we'll need to get a bigger conference room at a hotel.\", 2.9054781407661343: 'We are having our seminar in Bangkok, our first effortless English breakthrough seminar in Bangkok, Thailand on January 25th.', 2.850764896464866: \"And then I'm going to actually do some demonstration lessons, including a mini story lesson and a point of view lesson.\", 2.839695422542164: 'And remember the point of view lessons, those are the lessons where I teach grammar in a natural way, in an intuitive way.', 2.826037967953922: \"So we'll experience them live, the mini story lessons and the point of view lessons, which are the most powerful lesson types that we have.\", 2.817972023910201: \"So I'm going to talk about these new elements of the effortless English system.\", 2.800226389513972: 'And these new elements are especially powerful live in person with me because we can have a lot of interaction.', 2.777330404469449: \"Really, it's a half day seminar just to get a quick breakthrough with your English.\", 2.4788868576821432: 'So you know about the many stories, you know about using, you know, listen first and focusing on input.', 2.4647114732460937: \"We'll have another one in February and another one in March, also in Bangkok.\", 2.4479661980602003: 'I want to create something that is amazing, that is remarkable, that is outstanding.', 2.4393142773631107: \"We're going to have the first one January 25th on Sunday.\", 2.433948201349984: \"You're not just listening to me tell a story and ask questions.\", 2.423389196780132: 'So if you live in Thailand or you can get to Bangkok, check our website.', 2.3932177292101104: 'So if you live in Bangkok or in Southeast Asia, come to our seminars.', 2.376530628521342: \"So this January 25th starting at 1045 until 4 p.m. and that's on a Sunday, January 25th.\", 2.312050138866276: \"And that's across from Sanam Luang, across from the big field.\", 2.2856032897762777: \"You're actually shouting out answers and I change the story with your answers.\", 2.2785029774139285: \"It's a lot more fun when we're all together in a room because we create the stories together.\", 2.226826364718689: \"And I'll talk some about that too because some people might not know about it.\", 2.2246747625864973: \"That's the power of those point of view lessons.\", 2.219319996329345: \"But right now is a great opportunity because it's just a one day.\", 2.2166279063327154: \"But right now our first seminars we're doing in Bangkok, Thailand.\", 2.184834564975354: \"I'll announce the new seminars when we schedule them.\", 2.1807147095674186: \"I'm testing different parts of the seminar.\", 2.0757114476505008: \"There can be a lot more energy when we're all in the same room together, learning together.\", 1.998294414676501: 'Go to our website, effortlessenglishclub.com for more information.', 1.9976145766689293: 'It will be at the Royal Hotel, which is at number 2, Ratchadamnurn.', 1.9865086625283657: 'And our very first seminar is January 25th.', 1.9856765729003136: \"We'll have to pay a lot more money.\", 1.9835280862940992: \"There's a lot of energy in the room.\", 1.8874875601793666: \"Right now, the seminars are free because right now we're just developing them.\", 1.8240384867704147: 'So if you can come to one of our seminars, definitely come.', 1.7320508075688776: \"So it'll become more expensive.\", 1.7302724773813032: 'Okay, I hope to see you there.', 1.7301528918667914: 'See you next time.', 1.7147038167331208: \"And we'll do it live in person.\", 1.71159050073959: 'There may be some new people there.', 1.707576588213112: 'Learn with me personally, live.', 1.4142135623730951: 'Hello, this is AJ.', 1.4135137310269565: \"So that's great.\", 1.4125268133596947: 'More good news.', 1.4076976306225877: \"And it's free right now.\", 1.4040853699410754: \"It's a lot of fun.\", 1.0: 'Bye bye.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tdidf_sentences = list(t.values())[:10]\n",
        "tdidf_sentence = \" \".join(tdidf_sentences)\n",
        "tdidf_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "4qcM1dLrWzCg",
        "outputId": "caa782af-9d11-451b-cded-54da86352749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"And I'm going to talk about the general elements of the effortless English system, how to learn English most quickly, most easily, most effectively, how to speak easily, fast and fluently without thinking. For example, this summer we will definitely be coming to Japan and we'll be doing effortless English breakthrough seminars in Japan, probably in the Kansai area in Kyoto, Osaka, possibly Kobe. Now I'm telling everybody about this, even if you don't live in Bangkok, because we are developing these seminars so that in the future we can visit many cities around the world. Now, eventually, we will develop a longer seminar that will be possibly two, three, maybe four days, a very intensive English learning experience. But anyway, at these seminars what I'm going to do is number one, I'm going to talk about the effortless English system and especially about some new things that we are focusing on with our system. But I'm also going to talk about some of the new things that we are adding to our system in order to make the system even more powerful. Right now, the first one, January 25th, come to Bangkok and experience effortless English live. And have a great day and enjoy your English learning and start this new year with powerful English learning, powerful motivation, powerful goals. So you, the student, you never think about grammar rules, but you still learn the grammar, but you learn it like a native speaker. You'll have to pay because we'll need to get a bigger conference room at a hotel.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    }
  ]
}